# MapReduce

- Distributes the `processing of data` across the hadoop cluster
- Divides the data into partitions
  - Partitions are **mapped** (transformed) with a `mapper function`
  - Partitions are **shuffled** and **sorted** automatically
  - Partitions are **reduced** (aggregated) with a `reducer function`
- It's resilient to failure

## Map-Shuffle-Reduce Flow

- Raw data

| user_ud | movie_id | rating | timestamp   |
| ------- | -------- | ------ | ----------- |
| 1       | 200      | 5      | 82163892713 |
| 2       | 100      | 1      | 82163892713 |
| 3       | 300      | 2      | 82163892713 |
| 2       | 200      | 4      | 82163892713 |
| 1       | 300      | 5      | 82163892713 |

- Mapped data

```json
{
  "2": 100,
  "1": 200,
  "3": 300,
  "2": 200,
  "1": 300
}
```

- Shuffled data
  - Aggregate values with a same key (shuffle)
  - Sort by key

```json
{
  "1": [200, 300],
  "2": [100, 200],
  "3": [300]
}
```

- Reduced data
  - Calculate the number of movies rated by a user

```json
{
  "1": 2,
  "2": 2,
  "3": 1
}
```
