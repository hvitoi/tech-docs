# Apache Spark

- Engine for processing large-scale data (faster alternative to mapreduce)
- It's a way to distribute data processing loads across machines
- Divide and conquer approach
- Uses `DAG Engine` (directed acyclic graph) to optimize the workflow, like pig on tez. It finds the most efficient order to run tasks needed to accomplish the desired output
- Can be coded in `python`, `java` or `scala` (preferred, most efficient)
- It has a huge ecosystem on top of it for ML, AI, BI, etc
- Spark does not need to run on Hadoop. It also has its own built-in cluster manager

## Scalability

- `Driver Program`: It invokes spark context. Driver program is the script to be executed
- `Cluster Manager`: Spark or Yarn (handle how to distribute the processing)
- `Executor`: The workload. It's has cache and tasks

## Components

- `Spark Core`: RDDs definitions

- Additional libraries
  - `Spark Streaming`: ingest data as it's being produced
  - `Spark SQL`: SQL interface to spark. Spark as a huge relational database
  - `MLLib`: Machine Learning and Data Mining
  - `GraphX`: Graphs handling (e.g., network information, social networks)

## Spark server

- <http://spark.apache.org/>

```sh
curl -O "https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
```

## Spark Shell

- The `Spark Shell` creates a `sc` (spark context) object for you
- With the sc object you can interact with the RDDs
- RDDs can also be created manually by your script

```python
# RDD from in-memory object
nums = sc.parallelize([1, 2, 3, 4])

# RDD from a text file
nums = sc.textFile("file://home/user/nums.txt") # or s3n://, hdfs://

# RDD from hive context
nums = HiveContext(sc).sql("SELECT name,age FROM users")

# also from JBDC, Cassandra, HBase, Elasticsearch, CSV, etc
```
