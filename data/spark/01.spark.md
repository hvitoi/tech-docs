# Apache Spark

- Engine for processing large-scale data (faster alternative to mapreduce)
- Uses `DAG Engine` (directed acyclic graph) to optimize the workflow, like pig on tez
- Can be coded in `python`, `java` or `scala` (preferred)
- It has a huge ecosystem on top of it for ML, AI, BI, etc
- Spark does not need to run on Hadoop, it can run on its own

## Scalability

- `Driver Program`: Spark Context
- `Cluster Manager`: Spark or Yarn
- `Executor`: Cache, Tasks

## Components

- `Spark Core`
- `Spark Streaming`: ingest data as it's being produced
- `Spark SQL`: SQL interface to spark
- `MLLib`: Machine Learning and Data Mining
- `GraphX`: Graphs handling

## Spark server

- <http://spark.apache.org/>

```sh
curl -O "https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
```

## Spark Shell

- The `Spark Shell` creates a `sc` (spark context) object for you
- With the sc object you can interact with the RDDs
- RDDs can also be created manually by your script

```python
# RDD from in-memory object
nums = sc.parallelize([1, 2, 3, 4])

# RDD from a text file
nums = sc.textFile("file://home/user/nums.txt") # or s3n://, hdfs://

# RDD from hive context
nums = HiveContext(sc).sql("SELECT name,age FROM users")

# also from JBDC, Cassandra, HBase, Elasticsearch, CSV, etc
```
