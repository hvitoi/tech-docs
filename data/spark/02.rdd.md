# Resilient Distributed Dataset (RDD)

- Spark saves data in memory on RDD (`Resilient Distributed Dataset`)
- Spark tries to keep as much as possible on RAM (not on RDFS)
- Up to 100x faster than mapreduce
- For a programmer, RDD looks just like a dataset
- `SparkContext` is the environment in which the `RDDs` are available

## RDD transforms

- `map`: 1:1 relation (in/out)
- `flatmap`: any relationship between input/output
- `filter`: filter
- `distinct`: unique data
- `sample`: sample randomly
- `union, intersection, subtract, cartesian`: combine RDDs

```python
rdd = sc.parallelize([1, 2, 3, 4])
squaredRDD = rdd.map(lambda x: x*x) # [1, 4, 9, 16]
```

## RDD actions

- Only when an RDD action is called the data is processed
- Nothing actually happens in your driver program until an action is called (`lazy evaluation`7)

- `collect`: output the result and return an object
- `count`: count items in the RDD
- `countByValue`: count by unique value
- `take`: take n number of rows
- `top`: take the first rows
- `reduce`: combine the values
