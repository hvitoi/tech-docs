# Spark Context

- `SparkContext` is the environment in which the `RDDs` are available
- Spark Context object takes care automatically of the resilience of RDDs
  - By distributing them across machines
  - Support node failures

## Shell

- The `Spark Shell` creates a `sc` (spark context) object for you
- With the sc object you can interact with the RDDs
- RDDs can also be created manually by your script

```python
# RDD from in-memory object
nums = sc.parallelize([1, 2, 3, 4])

# RDD from a text file
nums = sc.textFile("file://home/user/nums.txt") # or s3n://, hdfs://

# RDD from hive context
nums = HiveContext(sc).sql("SELECT name,age FROM users")

# also from JBDC, Cassandra, HBase, Elasticsearch, CSV, etc
```

## Python (pyspark)

- Python library for accessing the spark context
- Under the hood python code has to be translated into java bytecode

## Scala (org.apache.spark)

- Scala library for accesing the spark context
- Scala is preferred over python because it's the most efficient
