from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, AIMessage, SystemMessage

# https://docs.langchain.com/oss/python/langchain/messages#message-prompts


# Roles
#   - System message: Tells the model how to behave and provide context for interactions
#   - Human message: Represents user input and interactions with the model. It can be text, images, etc
#   - AI message: Responses generated by the model, including text content, tool calls, and metadata
#   - Tool message: Represents the outputs of tool calls


def main():
    model = init_chat_model("google_genai:gemini-2.5-flash-lite")  # "gpt-5.2"

    messages = [
        SystemMessage("You are a helpful assistant"),
        HumanMessage("Can you help me?"),
        AIMessage("I'd be happy to help you with that question!"),
        HumanMessage("Great! What's 2+2?"),
    ]

    # Same
    messages2 = [
        ("system", "You are a helpful assistant"),
        ("user", "Can you help me?"),
        ("assistant", "I'd be happy to help you with that question!"),
        ("user", "Great! What's 2+2?"),
    ]

    # Same
    messages3 = [
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Can you help me?"},
        {"role": "assistant", "content": "I'd be happy to help you with that question!"},
        {"role": "user", "content": "Great! What's 2+2?"},
    ]

    response = model.invoke(messages)

    print(type(response))  # AIMessage
    print(response.content)

    # https://docs.langchain.com/oss/python/langchain/messages#text-prompts
    # Text prompts are strings, ideal for straightforward generation tasks where you don't need to retain conversation history.
    response = model.invoke("Write a haiku about spring")
    print(response.content)


if __name__ == "__main__":
    main()
