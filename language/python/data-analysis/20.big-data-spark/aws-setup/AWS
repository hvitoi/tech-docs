Amazon Web Services
https://aws.amazon.com/


Connect to Instance (Windows - PuTTY)
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html


Connect to Instance (Linux - SSH)
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html

chmod 400 sparkone.pem
ssh -i sparkone.pem ubuntu@ec2-52-67-196-115.sa-east-1.compute.amazonaws.com


PySpark Setup
https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297?

wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh
bash Anaconda3-2019.10-Linux-x86_64.sh
export PATH="/home/yash/anaconda3/bin:$PATH"
which python3
which python3

jupyter notebook --generate-config
mkdir certs
cd certs
sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem
cd ~/.jupyter/

vi jupyter_notebook_config.py
c = get_config()
# Notebook config this is where you saved your pem cert
c.NotebookApp.certfile = u'/home/ubuntu/anaconda3/certs/mycert.pem' 
# Run on all IP addresses of your instance
c.NotebookApp.ip = '*'
# Don't open browser by default
c.NotebookApp.open_browser = False  
# Fix port to 8888
c.NotebookApp.port = 8888
# Exit with :wq!

jupyter notebook

#access browser through
https://ec2-52-67-196-115.sa-east-1.compute.amazonaws.com:8888/


sudo apt-get update
sudo apt-get install default-jre
java -version
sudo apt-get install scala
scala -version

export PATH=$PATH:$HOME/anaconda3/bin
conda install pip
which pip
pip install py4j

wget https://archive.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop3.2.tgz
sudo tar -zxvf spark-3.0.0-preview-bin-hadoop3.2.tgz

export SPARK_HOME='/home/ubuntu/spark-3.0.0-preview-bin-hadoop3.2'
export PATH=$SPARK_HOME:$PATH
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

https://ec2-52-67-196-115.sa-east-1.compute.amazonaws.com:8888/


